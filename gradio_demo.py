# -*- coding: utf-8 -*-
# @Time    : 2025/3/13 15:05
# @Author  : yaomw
# @Desc    :
import time

import gradio as gr

from rag import RAGPipeline
from utils.ollama_utils import fetch_ollama_models
from utils.utils import get_file_list

from config import OllamaModelName

rag = RAGPipeline(OllamaModelName)

# å®šä¹‰å…¨å±€åœæ­¢æ ‡å¿—å˜é‡
stop_flag = False


def stop_chat():
    global stop_flag
    stop_flag = True
    # æ­¤å¤„å¯ä»¥è¿”å›ä¸€ä¸ªçŠ¶æ€ä¿¡æ¯ä¾›ç•Œé¢æ˜¾ç¤ºï¼Œä¹Ÿå¯ä»¥ä¸è¿”å›
    return "å›ç­”å·²ä¸­æ–­"


def process_upload_files(files):
    if not files:
        # è·å–esä¸­çš„æ–‡ä»¶åˆ—è¡¨
        file_list = get_file_list(rag.es_client, rag.es_index_name)
        
        # å·²å¤„ç†æ–‡ä»¶
        ret = [f"æ–‡ä»¶å: ã€Š{i['key']}ã€‹ï¼Œåˆ†å—æ•°é‡ï¼š{i['doc_count']}" for i in file_list]
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", "\n".join(ret)
    
    file_list = rag.load_and_split_documents(files=files)
    
    summary = f"\næ€»è®¡å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¤„ç†å®Œæˆ"
    
    # å·²å¤„ç†æ–‡ä»¶
    ret = [f"æ–‡ä»¶å: ã€Š{i['key']}ã€‹ï¼Œåˆ†å—æ•°é‡ï¼š{i['doc_count']}" for i in file_list]
    
    return summary, "\n".join(ret)


def process_chat(question, history, enable_web_search, model_choice):
    # åˆå§‹åŒ–å†å²è®°å½•
    
    print(
        f"\nragé—®ç­”å‚æ•°:\n question:{question}, history:{history}, è”ç½‘æœç´¢:{enable_web_search}, æ¨¡å‹é€‰æ‹©:{model_choice}")
    
    global stop_flag
    stop_flag = False  # é‡ç½®åœæ­¢æ ‡å¿—ï¼Œæ¯æ¬¡ç”Ÿæˆå‰ç¡®ä¿ä¸º False
    
    history = history or []
    
    if not question.strip():
        print("è¯·è¾“å…¥é—®é¢˜")
        yield history
        return
    
    # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
    history.append((question, ""))
    
    # ç«‹å³æ¸…ç©ºè¾“å…¥æ¡†å¹¶æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    yield history, ""
    
    chain = rag.setup_rag_chain(rerank_method="corom", enable_web_search=enable_web_search)
    
    # åˆå§‹åŒ–ç­”æ¡ˆ
    full_answer = ""
    
    thinking_mode = False  # æ˜¯å¦å¤„äºæ€è€ƒè¿‡ç¨‹
    
    think_time = time.time()
    
    # æµå¼è·å–å›ç­”
    for chunk in chain.stream(question):
        
        print(chunk, end="", flush=True)
        
        if stop_flag:  # æ£€æŸ¥æ˜¯å¦ç‚¹å‡»äº†åœæ­¢æŒ‰é’®
            full_answer += "\n\nå›ç­”å·²ä¸­æ–­"
            history[-1] = (question, full_answer)
            yield history, ""
            return  # ä¸­æ–­åç›´æ¥ç»“æŸ
        
        full_answer += chunk
        
        # æ£€æµ‹æ€è€ƒè¿‡ç¨‹å¼€å§‹ï¼Œç¡®ä¿åªè§¦å‘ä¸€æ¬¡
        if "<think>" in full_answer and not thinking_mode:
            thinking_mode = True
            # ç”¨ <details open> å’Œ <summary>æ€è€ƒä¸­...</summary> æ›¿æ¢ <think>
            full_answer = full_answer.replace(
                "<think>",
                "<details open>\n<summary>æ€è€ƒä¸­...</summary>\n"
            )
        
        # å¦‚æœåœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»“æŸæ ‡è®°
        if thinking_mode and "</think>" in full_answer:
            # å°† </think> æ›¿æ¢ä¸º </details> å¹¶æ›´æ–° summary æ–‡æœ¬
            full_answer = full_answer.replace("</think>",
                                              f"\n</details>\n\n ----------æ€è€ƒè€—æ—¶:{(time.time() - think_time):.2f}s----------")
            full_answer = full_answer.replace("æ€è€ƒä¸­...", "æ€è€ƒå®Œæˆ")
            thinking_mode = False
        
        # æ›´æ–°æœ€åä¸€æ¡å†å²è®°å½•
        history[-1] = (question, full_answer)
        # time.sleep(0.02)
        # é€æ­¥è¿”å›æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        yield history, ""
    
    # å¦‚æœå¾ªç¯æ­£å¸¸ç»“æŸï¼Œä¹Ÿç¡®ä¿è¿”å›æœ€ç»ˆç»“æœ
    yield history, ""


def clear_chat_history():
    return None, "å¯¹è¯å·²æ¸…ç©º"


with gr.Blocks() as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Tabs() as tabs:
        # ç¬¬ä¸€ä¸ªé€‰é¡¹å¡ï¼šé—®ç­”å¯¹è¯
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ“ä½œé¢æ¿ - è°ƒæ•´æ¯”ä¾‹ä¸ºåˆé€‚çš„å¤§å°
                with gr.Column(scale=5, elem_classes="left-panel"):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        file_input = gr.File(
                            label="ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒpdfã€docxã€txtã€mdã€html",
                            file_types=[],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False,
                            lines=2
                        )
                        file_list = gr.Textbox(
                            label="å·²å¤„ç†æ–‡ä»¶",
                            interactive=False,
                            lines=3,
                            elem_classes="file-list"
                        )
                    
                    # å°†é—®é¢˜è¾“å…¥åŒºç§»è‡³å·¦ä¾§é¢æ¿åº•éƒ¨
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            # æ·»åŠ è”ç½‘å¼€å…³
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢",
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€è¦VPNï¼‰"
                            )
                            
                            # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                            model_choice = gr.Dropdown(
                                choices=fetch_ollama_models(),
                                value="deepseek-r1:14b",
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                            )
                        
                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button",
                                                  scale=1)
                
                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
                    
                    # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=600,  # å¢åŠ é«˜åº¦
                        elem_classes="chat-container",
                        show_label=False
                    )
                    
                    # åœ¨å¯¹è¯è®°å½•åŒºåŸŸä¸­æ·»åŠ åœæ­¢æŒ‰é’®
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢å›ç­”", variant="secondary")
                    
                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)
    
    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_upload_files,
        inputs=[file_input],
        outputs=[upload_status, file_list]
    )
    
    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input]
    )
    
    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )
    
    # ç»‘å®šåœæ­¢æŒ‰é’®äº‹ä»¶ï¼Œä¸­æ–­æµå¼å›ç­”
    stop_btn.click(
        stop_chat,
        inputs=[],
        outputs=[]
    )

if __name__ == "__main__":
    demo.queue(max_size=5).launch(server_name="0.0.0.0", server_port=7860)