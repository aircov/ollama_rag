# -*- coding: utf-8 -*-
# @Time    : 2025/3/13 15:05
# @Author  : yaomw
# @Desc    : gradioç•Œé¢
import time

import gradio as gr

from async_rag import AsyncRAGPipeline
from utils.ollama_utils import fetch_ollama_models
from utils.utils import get_file_list

from config import OllamaModelName
from extensions import logger

rag = AsyncRAGPipeline()


async def process_upload_files(files):
    if not files:
        # è·å–esä¸­çš„æ–‡ä»¶åˆ—è¡¨
        file_list = await get_file_list(rag.es_client, rag.es_index_name)
        
        # å·²å¤„ç†æ–‡ä»¶
        ret = [f"æ–‡ä»¶å: ã€Š{i['key']}ã€‹ï¼Œåˆ†å—æ•°é‡ï¼š{i['doc_count']}" for i in file_list]
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", "\n".join(ret)
    
    file_list = await rag.load_and_split_documents(files=files)
    
    summary = f"\næ€»è®¡å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¤„ç†å®Œæˆ"
    
    # å·²å¤„ç†æ–‡ä»¶
    ret = [f"æ–‡ä»¶å: ã€Š{i['key']}ã€‹ï¼Œåˆ†å—æ•°é‡ï¼š{i['doc_count']}" for i in file_list]
    
    return summary, "\n".join(ret)


async def process_chat(question, history, enable_web_search, model_choice):
    """
    :param question:
    :param history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ [(ç”¨æˆ·é—®é¢˜1, AIå›ç­”1), (ç”¨æˆ·é—®é¢˜2, AIå›ç­”2), ...]
    :param enable_web_search:
    :param model_choice:
    :return:
    """
    
    logger.info(
        f"\nragé—®ç­”å‚æ•°:\n question:{question}, history:{history}, è”ç½‘æœç´¢:{enable_web_search}, æ¨¡å‹é€‰æ‹©:{model_choice}")
    
    global stop_flag
    stop_flag = False  # é‡ç½®åœæ­¢æ ‡å¿—ï¼Œæ¯æ¬¡ç”Ÿæˆå‰ç¡®ä¿ä¸º False
    
    history = history or []
    
    if not question.strip():
        history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
        yield history, ""
        return
    
    # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
    history.append((question, ""))
    
    # ç«‹å³æ¸…ç©ºè¾“å…¥æ¡†å¹¶æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    yield history, ""
    
    # åˆå§‹åŒ–ç­”æ¡ˆ
    full_answer = ""
    
    thinking_mode = False  # æ˜¯å¦å¤„äºæ€è€ƒè¿‡ç¨‹
    
    think_time = time.time()
    
    try:
        # æµå¼è·å–å›ç­”
        async for chunk in rag.generate_answer(question, history=history, rerank_method="corom",
                                               enable_web_search=enable_web_search, max_turns=20,
                                               model_name=model_choice):
            print(chunk, end="", flush=True)
            
            if stop_flag:  # æ£€æŸ¥æ˜¯å¦ç‚¹å‡»äº†åœæ­¢æŒ‰é’®
                full_answer += "\n\nå›ç­”å·²ä¸­æ–­"
                history[-1] = (question, full_answer)
                yield history, ""
                break  # ä¸­æ–­åç›´æ¥ç»“æŸ
            
            full_answer += chunk
            
            # æ£€æµ‹æ€è€ƒè¿‡ç¨‹å¼€å§‹ï¼Œç¡®ä¿åªè§¦å‘ä¸€æ¬¡
            if "<think>" in full_answer and not thinking_mode:
                thinking_mode = True
                # ç”¨ <details open> å’Œ <summary>æ€è€ƒä¸­...</summary> æ›¿æ¢ <think>
                full_answer = full_answer.replace(
                    "<think>",
                    "<details open>\n<summary>æ€è€ƒä¸­...</summary>\n"
                )
            
            # å¦‚æœåœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»“æŸæ ‡è®°
            if thinking_mode and "</think>" in full_answer:
                # å°† </think> æ›¿æ¢ä¸º </details> å¹¶æ›´æ–° summary æ–‡æœ¬
                full_answer = full_answer.replace("</think>",
                                                  f"\n</details>\n\n ----------æ€è€ƒè€—æ—¶:{(time.time() - think_time):.2f}s----------")
                full_answer = full_answer.replace("æ€è€ƒä¸­...", "æ€è€ƒå®Œæˆ")
                thinking_mode = False
            
            # æ›´æ–°æœ€åä¸€æ¡å†å²è®°å½•
            history[-1] = (question, full_answer)
            # time.sleep(0.02)
            # é€æ­¥è¿”å›æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
            yield history, ""
        
        # å¦‚æœå¾ªç¯æ­£å¸¸ç»“æŸï¼Œä¹Ÿç¡®ä¿è¿”å›æœ€ç»ˆç»“æœ
        yield history, ""
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        history[-1] = (question, "ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯")
        yield history, ""


def clear_chat_history():
    return None, "å¯¹è¯å·²æ¸…ç©º"


with gr.Blocks() as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Tabs():
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                with gr.Column(scale=5):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        file_input = gr.File(
                            label="ä¸Šä¼ æ–‡æ¡£ï¼Œæ”¯æŒ pdfã€docxã€txtã€mdã€html",
                            file_types=[],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False, lines=2)
                        file_list = gr.Textbox(label="å·²å¤„ç†æ–‡ä»¶", interactive=False, lines=3)
                    
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜ï¼Œç¤ºä¾‹ï¼šå¦‚ä½•å®ç°å¿«é€Ÿæ’åºï¼Ÿå†™ä¸€ä¸ªpythonä»£ç ",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢",
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹"
                            )
                            model_choice = gr.Dropdown(
                                choices=fetch_ollama_models(),
                                value=OllamaModelName,
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹"
                            )
                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button",
                                                  scale=1)
                
                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
                    chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=600, show_label=False)
                    # stop_btn = gr.Button("â¹ï¸ åœæ­¢å›ç­”", variant="secondary")
                    btn_stop = gr.Button("åœæ­¢å›ç­”")
                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)
    
    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_upload_files,
        inputs=[file_input],
        outputs=[upload_status, file_list]
    )
    
    # ç»‘å®šæé—®æŒ‰é’®
    submit_event = ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input]
    )
    
    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )
    
    # ç»‘å®šåœæ­¢æŒ‰é’®äº‹ä»¶ï¼Œä¸­æ–­æµå¼å›ç­”
    # stop_btn.click(
    #     stop_chat,
    #     inputs=[],
    #     outputs=[]
    # )
    
    # åœæ­¢æŒ‰é’®å–æ¶ˆå½“å‰å¼‚æ­¥ä»»åŠ¡
    btn_stop.click(
        lambda: gr.update(),
        inputs=None,
        outputs=None,
        cancels=[submit_event]
    )

if __name__ == "__main__":
    demo.queue(default_concurrency_limit=10).launch(server_name="0.0.0.0", server_port=7860)
